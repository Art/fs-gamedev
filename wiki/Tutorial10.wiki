#summary Parsing DirectX' .X files.

== Introduction ==

This tutorial shows how to parse .X files, which are used for 3d models and scenes. Note that DirectX offers convenient functions to manipulate these files, meaning that the practical usefulness of the code in this tutorial is limited, unless you don't have access to DirectX.

Be aware that the code in this tutorial is not fully functional. For instance, error reporting is kept to a minimum (error *detecting* is handled, though)

This tutorial demonstrates the following concepts:
  * Discriminated unions and simple pattern matching
  * Stateless lexing
  * Hiding side effects in IO operations
  * Writing a lexer using fslex
  * Error handling using the `option` type
  * Writing a parser manually
  * Building functions at run-time by combining basic functions

The content of this tutorial is somewhat more dense than usual.

=== The .X format ===

A fairly comprehensive description of the file format is available on MSDN or on http://ozviz.wasp.uwa.edu.au/~pbourke/dataformats/directx/.

An .X file can basically divided into two parts. The first declares data structures, the second defines data, which structured according to the declarations in the first part.

=== Parser Architecture ===

There are several options for the design of the parser:
  # Ignore the data structures declared in the first part, and assume they match the standard definitions by Microsoft; then parse the second part using a "hard-wired" data parser.
  # Parse the data structure declarations, producing data which is used when parsing the second part.
  # Parse the data structure declarations, producing parsers which are used to parse the second part.

I find the third option to be the most attractive, mostly for aesthetic reasons. It is therefore the one used for the code of this tutorial.

== Code ==

The full code is available at http://code.google.com/p/fs-gamedev/source/browse/#svn/trunk/Tutorial013.

It is organized into the following modules:
  * *XLexers.fs* The interface of lexers, i.e. functions turning bytes read from a file into data sent to the parser.
  * *TextLexer.fs* An implementation of a lexer which extracts data from text files. .X files may also be binary files, but this case is not handled in this tutorial. *TextLexer.fs* is automatically generated by fslex from *TextLexer.fsl*.
  * *XParsers.fs* The parser.
  * *Program.fs* Entry point for the application.

The rest of this section illustrates each concept.

=== Discriminated unions ===

From XLexers.FS:

{{{
type Token =
    | NAME of string
    | STRING of string 
    | INTEGER of int
    | FLOATVAL of float
    | UUID of string
    | INTEGER_LIST of int list   // Not used ?!
    | FLOAT_LIST of float list   // Not used ?!
    | OBRACE | CBRACE | OPAREN | CPAREN
    | OBRACKET | CBRACKET | OANGLE | CANGLE
    | DOT | COMMA | SEMICOLON
    | TEMPLATE
    | WORD | DWORD
    | FLOAT | DOUBLE
    | CHAR | UCHAR
    | SWORD | SDWORD
    | VOID
    | LPSTR
    | UNICODE
    | CSTRING
    | NSTRING
    | ARRAY
    | EOF

}}}

This defines a _discriminated union_. A variable of type `Token` can be a `NAME`, in which case its value is a `string`, or a `INTEGER`, in which case its value is an `int`, and so on...

`NAME` and `INTEGER` are called _discriminators_, and are used to identify the actual type of a variable of type `Token`.

=== Stateless lexing ===

From XLexers.FS:

{{{
type Lexer<'Src> = 'Src -> (Token * 'Src) option
}}}

Another type definition: This is the signature of functions turning file contents into tokens. The type `Lexer` is parametrized by the type of the source from which tokens are extracted. A `Lexer` takes a source, and returns:
  * a pair of a token and a new source, or
  * nothing, if there was an error while reading the source, or if the source is ill-formatted.

Typically, the source contains information about the file being read, and where the 'head' points inside the file.
The lexer is expected to return the source it was passed, with the difference that the new position points after the token that was just read.

{{{
let expect nextToken (src : 'Src) (tokens : Token list) =
    let rec work src tokens =
        match tokens with
        | [] -> Some src
        | tok :: toks ->
            match nextToken src with
            | Some(head, rest) when head = tok -> work rest toks
            | _ -> None
    work src tokens
}}}

The function `expect` takes a lexer (somewhat misleadingly called `nextToken`), a source and a list of expected tokens. It checks that the sequence of tokens extracted from the source matches the expected list, then returns the source, pointing after the sequence of tokens. If the lexer returned a token which was not expected, or reached the end of the source too early, `None` is returned.

You may wonder why `nextToken` returns a modified copy of the source it received. The traditional way of doing things modifies the source being passed as an argument, and returns the parsed token or an error code. The method used here avoids the side effect, which is both elegant and practical if you want to be able to do something like:

{{{
   (* Get next token, but ignore SomeTokenToIgnore *)
   let token, source = nextToken in_file
   let really_token, source2 =
      if token == SomeTokenToIgnore then nextToken source
      else token, source
}}}

Another example:

{{{
   (* For backward compatibility reasons, try to parse using the old parser.
      If we fail, use the new parser, which uses a different syntax *)
   let parse_attempt1 = oldParser in_file
   let final_attempt =
      match parse_attempt1 with
      | None -> newParser in_file
      | _ -> parse_attempt1
}}}

You can do that because `in_file` does not change after calling `nextToken`.

There is an open problem, though. Designing the interface of a lexer that encapsulates the state of the file from which it reads is easy, but may well be unusable. After all, most IO APIs are based on side-effects. The next section shows a solution to this problem.

=== Hiding side effects in IO operations ===

From Program.fs:

{{{
open TextLexer
open System.IO
open Microsoft.FSharp.Text.Lexing

let tokenStream filename =
    seq {
        use stream = File.OpenRead(filename)
        use reader = new StreamReader(stream)
        reader.ReadLine() |> ignore
        let lexbuf = Lexing.from_text_reader (new System.Text.ASCIIEncoding()) reader
        while not lexbuf.IsPastEndOfStream do
            match TextLexer.token lexbuf with
            | XLexers.EOF as value -> yield! [value]
            | token -> yield token
        }
    |> LazyList.of_seq
}}}

This function takes the name of file to read.

It creates a _sequence expression_, that is a piece of code that generates items in a sequence. Sequences are abstract types which can be converted to concrete lists, arrays. They can also be evaluated lazily, which means that the content of the sequence is not instantiated at once when the sequence is created. Instead, items are created on demand.
In the example above, the sequence is turned into a lazy list. Lazy lists are similar to normal lists, with the difference that items are created when needed.

This sequence expression does the following:
  # Opens the file to read
  # Creates a stream reader to read from this file
  # The first line is retrieved, and ignored
  # A lexer, generated using fslex, is instantiated; it will read from the stream reader
  # The lexer reads tokens one by one, building a sequence of tokens, until the end of the file is reached

Individual items are produced using `yield`, chunks of items can be produced at once using `yield!`.

Note that the last step does not actually read the entire file at once. Instead, each iteration will be executed when a new token is needed, at which point execution continues in the main program.

The type `seq` (which is instantiated by the sequence expression) is similar to the type `list`. It has operations allowing to retrieve items, but unlike `list`, it does not allow pattern-matching. This minor issue is easily solved by converting the sequence to a `LazyList`. The following is now possible:

{{{
// A lexer working on a lazy list of Tokens. The list is constructed on the fly as the file is read
let nextTokenLazyList (debug : bool) (src : LazyList<XLexers.Token>) =
    match src with
    | LazyList.Nil -> None
    | LazyList.Cons(head, rest) ->
        if debug then printfn "%A" head;
        Some(head, rest)
}}}

This function takes an argument controlling whether debug info is printed, and a lazy list of tokens from which tokens are read and returned, one by one.

Note that calling `nextTokenLazyList` twice with the same argument will produce the same result, as one would expect from a pure function.
   
=== Writing a lexer using fslex ===

See TextLexer.fsl for an example. I won't go into further details here. If you are familiar with lex you should not have difficulties sorting out this file. Otherwise, you may want to look into one of the many tutorials available on lex on the net, for instance http://epaperpress.com/lexandyacc/. This tool was originally written for the C programming language, and derivatives were made for almost all new programming languages that came after C.

=== Error handling using the `option` type ===

A common way to handle errors is to use the special value `null`. For instance, functions allocating memory return a reference to a newly allocated chunk of memory, or `null` if the allocation failed, maybe due to a lack of available memory. Although convenient, this trick has introduced a family of bugs, so-called null-pointer exceptions, which occur when trying to use a reference which is actually `null`. 

The `option` type in F# is used where the special value `null` is typically used in other languages. It can be seen as a parametrized discriminated union:

{{{
type Option<'T> =
  | None
  | Some of 'T
}}}

For efficiency reasons, the F# compiler replaces instances of `option` by `null` and valid references.

The point of `option` is that it makes it possible to write code that is safe with respect to null-pointer exceptions.

Here is an example illustrating the use of `option`:
{{{
// Parse the entire file.
let parse (lexer : LexerFuncs<'Src>) src : (string option * Record) list option =
    // Function parsing all templates
    let rec loop src parsers =
        match parseTemplate lexer src parsers with
        | Some(parsers, src) -> loop src parsers
        | None -> parsers, src
    
    // First parse all templates
    let env, src = loop src Map.empty
    
    // Then parse the data
    let r = parseData lexer src env
    match r |> lexer.MaybeExpect [EOF] with
    | Some(value, _) -> Some value
    | None -> None
}}}

This function is the top-level parsing function. It parses the first part of an .X file, which contains the declarations of data structures. The .X file format does not have an explicit separator between the two parts, which means that one basically has to try and read the first part until it fails, at which point an attempt to read the second part can be made.

The local function `loop` repeatedly reads data from the file, parsing the data it reads as data structure declarations. When `parseTemplate`, the function parsing data structures, returns `None`, `loop` returns the result of the successful part of parsing.

The rest of `parse` continues the parsing process, _where successful parsing of data structures stopped_. Note that this would have been trickier to do using a stateful lexer and file reader: By the time `parseTemplate` notices that it's not parsing a data structure declaration, several tokens may already been read and discarded. To solve this problem, one must leave a 'bookmark' in the file reader after each successful parsing of a data structure declaration, which is easily and elegantly done when using stateless lexing: just keep around the `src` variable used when parsing failed.

=== Writing a parser manually ===

=== Building functions at run time ===

== Conclusion ==